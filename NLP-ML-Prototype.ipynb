{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting gender from blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with an 80k subsample. This gives us a large enough sample size to fully train the models, but small enough to work with locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "df = pd.read_csv('blogtext.csv', parse_dates = ['date']).sample(80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/drew/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['custom_topic'].value_counts().plot('bar')\n",
    "import spacy\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "stops = [spacy.lang.en.stop_words.STOP_WORDS] + stopwords.words('english')\n",
    "def clean_jv(doc):\n",
    "   typo_free = ' '.join([(sym_spell.lookup(i, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)[0].term) for i in doc])\n",
    "   twol_free = strip_short(typo_free)\n",
    "   return twol_free\n",
    "def clean(lst):\n",
    "    lst = [token for token in lst if not token.is_stop]\n",
    "    lst = [token.lemma_ for token in lst if str(token.lemma_) not in stops]\n",
    "    lst = [re.sub(r'[\\W\\d\\s]', '', string) for string in lst]\n",
    "    lst = [token for token in lst if token not in stops]\n",
    "    while '' in lst:\n",
    "        lst.remove('')\n",
    "    lst = clean_jv(lst)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using gensim to create bigrams and trigrams here\n",
    "df['tokens'] = df['text'].map(lambda x: nlp.tokenizer(x.lower()))\n",
    "df['tokens'] = df['tokens'].map(lambda x: clean(x).split())\n",
    "#Creating a single-string version of the cleaned texts, for tools that require it.\n",
    "df['token_str'] = df['tokens'].map(lambda x: ' '.join(x))\n",
    "doc_clean = df['tokens']\n",
    "bigram =gensim.models.Phrases(doc_clean, min_count = 5, threshold = 50)\n",
    "trigram =gensim.models.Phrases(bigram[doc_clean], min_count = 5, threshold = 50)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "def make_trigrams(texts):\n",
    "    return trigram_mod[bigram_mod[texts]]\n",
    "doc_trigram = df['tokens'].map(lambda x: make_trigrams(x))\n",
    "dct = gensim.corpora.Dictionary(doc_trigram)\n",
    "dct.filter_extremes(no_below=5, no_above=0.6666, keep_n=90000)\n",
    "doc_term_matrix = [dct.doc2bow(doc) for doc in doc_trigram]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "y=le.fit_transform(df['gender'])\n",
    "X=df['token_str']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(lowercase=True, analyzer = \"word\",\n",
    "#                      max_features=3500,min_df=4, tokenizer=None, \n",
    "#                      ngram_range = [3,5], preprocessor=None)\n",
    "# X_train_cv = cv.fit_transform(X_train)\n",
    "# X_test_cv = cv.transform(X_test)\n",
    "#Metrics I'm going to use\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import TfIdfTransformer\n",
    "from gensim.corpora import Dictionary\n",
    "X = df['tokens']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "dictionary = Dictionary(X_train)\n",
    "model = TfIdfTransformer(dictionary)\n",
    "train = [dictionary.doc2bow(text) for text in X_train]\n",
    "test = [dictionary.doc2bow(text) for text in X_test]\n",
    "X_train_g =  model.fit_transform(train)\n",
    "X_test_g = model.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn's tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfid = TfidfVectorizer(use_idf=True, min_df=3, analyzer='word', smooth_idf=True,\n",
    "                       norm = 'l2', ngram_range=[3,5], sublinear_tf=True)\n",
    "# X=df['form_str']\n",
    "X=df['token_str']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "tfid_train_matrix = tfid.fit_transform(X_train)\n",
    "tfid_test_matrix = tfid.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5048\n",
      "F1 score:  0.664839255499154\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier(num_boosting_rounds=300, learning_rate=.12, num_parallel_trees=2)\n",
    "clf.fit(tfid_train_matrix, y_train)\n",
    "y_preds=clf.predict(tfid_test_matrix)\n",
    "print('Accuracy score: ', accuracy_score(y_test, y_preds))\n",
    "print(\"F1 score: \", f1_score(y_test , y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
