{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline\n",
    "### Want one function (eventually) to clean up text, create a dictionary, and return a DataFrame efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style='darkgrid')\n",
    "import spacy\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ListingId</th>\n",
       "      <th>PublicRemarks</th>\n",
       "      <th>MLSListingID</th>\n",
       "      <th>ListingBrokerName</th>\n",
       "      <th>ailPropertyID</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>ClosedPrice</th>\n",
       "      <th>avmValue</th>\n",
       "      <th>avmCompMatchLevel</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_0.0</th>\n",
       "      <th>topic_1.0</th>\n",
       "      <th>topic_2.0</th>\n",
       "      <th>topic_3.0</th>\n",
       "      <th>topic_4.0</th>\n",
       "      <th>topic_5.0</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Residual</th>\n",
       "      <th>perc_improved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>200488922</td>\n",
       "      <td>Charming Brick Home. 4 bedrooms, 2 bath. This ...</td>\n",
       "      <td>1588623</td>\n",
       "      <td>Patricia K Green</td>\n",
       "      <td>77369181</td>\n",
       "      <td>2019-04-30</td>\n",
       "      <td>215000.0</td>\n",
       "      <td>216270.82</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.240286</td>\n",
       "      <td>0.234410</td>\n",
       "      <td>-3889.238059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>200609063</td>\n",
       "      <td>SINGLE LEVEL HOME updated throughout. VERY cle...</td>\n",
       "      <td>1594269</td>\n",
       "      <td>Jeffrey Boyson</td>\n",
       "      <td>77384141</td>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>242000.0</td>\n",
       "      <td>241906.44</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.064723</td>\n",
       "      <td>0.064336</td>\n",
       "      <td>-16534.500592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>200691727</td>\n",
       "      <td>This beautiful newly renovated home has an ope...</td>\n",
       "      <td>1595726</td>\n",
       "      <td>Erica Peterson</td>\n",
       "      <td>76847035</td>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>290000.0</td>\n",
       "      <td>252066.80</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150489</td>\n",
       "      <td>0.072125</td>\n",
       "      <td>0.078364</td>\n",
       "      <td>47.926967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>200721083</td>\n",
       "      <td>This North Ogden beauty has features even bett...</td>\n",
       "      <td>1596255</td>\n",
       "      <td>Ashley Wolthuis</td>\n",
       "      <td>77342957</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>258307.76</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.093945</td>\n",
       "      <td>0.081139</td>\n",
       "      <td>-533.626801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>201121130</td>\n",
       "      <td>**MULTIPLE OFFERS RECEIVED.  NO MORE SHOWINGS ...</td>\n",
       "      <td>1598584</td>\n",
       "      <td>Nathan Poulsen</td>\n",
       "      <td>77293443</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>207384.94</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060829</td>\n",
       "      <td>0.129453</td>\n",
       "      <td>0.068624</td>\n",
       "      <td>-12.813917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ListingId                                      PublicRemarks  \\\n",
       "0           8  200488922  Charming Brick Home. 4 bedrooms, 2 bath. This ...   \n",
       "1          23  200609063  SINGLE LEVEL HOME updated throughout. VERY cle...   \n",
       "2          34  200691727  This beautiful newly renovated home has an ope...   \n",
       "3          39  200721083  This North Ogden beauty has features even bett...   \n",
       "4          53  201121130  **MULTIPLE OFFERS RECEIVED.  NO MORE SHOWINGS ...   \n",
       "\n",
       "  MLSListingID ListingBrokerName  ailPropertyID  ClosedDate  ClosedPrice  \\\n",
       "0      1588623  Patricia K Green       77369181  2019-04-30     215000.0   \n",
       "1      1594269    Jeffrey Boyson       77384141  2019-05-20     242000.0   \n",
       "2      1595726    Erica Peterson       76847035  2019-06-05     290000.0   \n",
       "3      1596255   Ashley Wolthuis       77342957  2019-05-31     255000.0   \n",
       "4      1598584    Nathan Poulsen       77293443  2019-07-11     220000.0   \n",
       "\n",
       "    avmValue  avmCompMatchLevel  ...  topic_0.0  topic_1.0 topic_2.0  \\\n",
       "0  216270.82                  6  ...          0          0         0   \n",
       "1  241906.44                  5  ...          0          0         0   \n",
       "2  252066.80                  6  ...          0          0         0   \n",
       "3  258307.76                 10  ...          0          1         0   \n",
       "4  207384.94                 10  ...          0          0         0   \n",
       "\n",
       "   topic_3.0  topic_4.0  topic_5.0    Actual  Predicted  Residual  \\\n",
       "0          0          0          1  0.005876   0.240286  0.234410   \n",
       "1          1          0          0  0.000387   0.064723  0.064336   \n",
       "2          1          0          0  0.150489   0.072125  0.078364   \n",
       "3          0          0          0  0.012805   0.093945  0.081139   \n",
       "4          1          0          0  0.060829   0.129453  0.068624   \n",
       "\n",
       "   perc_improved  \n",
       "0   -3889.238059  \n",
       "1  -16534.500592  \n",
       "2      47.926967  \n",
       "3    -533.626801  \n",
       "4     -12.813917  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('full.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what the topics are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5190946781513662"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import logging\n",
    "import gensim\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "df['PublicRemarks']=df['PublicRemarks'].astype(str)\n",
    "df['tokens'] = df['PublicRemarks'].map(lambda x: nlp.tokenizer(x.lower()))\n",
    "\n",
    "stops = [spacy.lang.en.stop_words.STOP_WORDS] + stopwords.words('english') + ['bath','baths','bed','beds','bedroom', 'bedrooms', \n",
    "                                                                     'property', 'home', 'lot', 'room', 'kitchen', 'mstr',\n",
    "                                                                     'master', 'bath', 'floor', 'main', 'level', \n",
    "                                                                     'living', 'room', 'home', 'area', 'w'] + ['able', 'about', 'above', 'abst', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'added', 'adj', 'affected', 'affecting',\n",
    "                    'affects', 'after', 'afterwards', 'again', 'against', 'ah', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among',\n",
    "                   'amongst', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apparently',\n",
    "                   'approximately', 'are', 'aren', 'arent', 'arise', 'around', 'as', 'aside', 'ask', 'asking', 'at', 'auth', 'away', 'awfully', 'back',\n",
    "                   'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'beginning', 'beginnings', 'begins', 'behind',\n",
    "                   'being', 'believe', 'below', 'beside', 'besides', 'between', 'beyond', 'biol', 'both', 'brief', 'briefly', 'but', 'by', 'c', 'ca', 'came', 'can',\n",
    "                   'cannot', 'can''t', 'cause', 'causes', 'certain', 'certainly', 'co', 'com', 'come', 'comes', 'contain', 'containing', 'contains', 'could', 'couldnt',\n",
    "                   'date', 'did', 'didn''t', 'different', 'do', 'does', 'doesn''t', 'doing', 'done', 'don''t', 'down', 'downwards', 'due', 'during', 'each', 'ed', 'edu',\n",
    "                   'effect', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'especially', 'et', 'et-al', 'etc', 'even', 'ever', 'every',\n",
    "                   'everybody', 'everyone', 'everything', 'everywhere', 'except', 'far', 'few', 'ff', 'fifth', 'first', 'five', 'followed', 'following', 'follows',\n",
    "                   'for', 'former', 'formerly', 'forth', 'found', 'four', 'from', 'further', 'furthermore', 'gave', 'get', 'gets', 'getting', 'give', 'given', 'gives',\n",
    "                   'giving', 'go', 'goes', 'gone', 'got', 'gotten', 'had', 'happens', 'hardly', 'has', 'hasn''t', 'have', 'haven''t', 'having', 'he', 'hed', 'hence', 'her',\n",
    "                   'here', 'hereafter', 'hereby', 'herein', 'heres', 'hereupon', 'hers', 'herself', 'hes', 'hi', 'hid', 'him', 'himself', 'his', 'hither', 'home', 'how',\n",
    "                   'howbeit', 'however', 'hundred', 'id', 'ie', 'if', 'im', 'importance', 'important', 'in', 'inc', 'indeed', 'index',\n",
    "                   'information', 'instead', 'into', 'invention', 'inward', 'is', 'isn''t', 'it', 'itd', 'it''ll', 'its', 'itself', 'just', 'keep', 'keeps', 'kept',\n",
    "                   'kg', 'km', 'know', 'known', 'knows', 'largely', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'lets', 'like', 'liked',\n",
    "                   'likely', 'line', 'little', 'look', 'looking', 'looks', 'ltd', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', 'me', 'mean', 'means', 'meantime',\n",
    "                   'meanwhile', 'merely', 'mg', 'might', 'million', 'miss', 'ml', 'more', 'moreover', 'most', 'mostly', 'much', 'mug', 'my', 'myself', 'name', 'namely', 'nay',\n",
    "                   'near', 'nearly', 'necessarily', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non',\n",
    "                   'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'now', 'nowhere', 'obtain', 'obtained', 'obviously', 'of', 'off', 'often',\n",
    "                   'oh', 'ok', 'okay', 'old', 'omitted', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'ord', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves',\n",
    "                   'out', 'outside', 'over', 'overall', 'owing', 'own', 'page', 'pages', 'part', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus',\n",
    "                   'poorly', 'possible', 'possibly', 'potentially', 'predominantly', 'present', 'previously', 'primarily', 'probably', 'promptly', 'proud', 'provides', 'put', 'que',\n",
    "                   'quickly', 'quite', 'qv', 'ran', 'rather', 'rd', 're', 'readily', 'really', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related',\n",
    "                   'relatively', 'research', 'respectively', 'resulted', 'resulting', 'results', 'right', 'run', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sec', 'section',\n",
    "                   'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sent', 'seven', 'several', 'shall', 'she', 'shed', 'she''ll', 'shes', 'should',\n",
    "                   'shouldn''t', 'show', 'showed', 'shown', 'showns', 'shows', 'significant', 'significantly', 'similar', 'similarly', 'since', 'six', 'slightly', 'so', 'some',\n",
    "                   'somebody', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specifically', 'specified', 'specify',\n",
    "                   'specifying', 'still', 'stop', 'strongly', 'sub', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure', 'take', 'taken', 'taking',\n",
    "                   'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'that''ll', 'thats', 'that''ve', 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "                   'thence', 'there', 'thereafter', 'thereby', 'thered', 'therefore', 'therein', 'there''ll', 'thereof', 'therere', 'theres', 'thereto', 'thereupon', 'there''ve',\n",
    "                   'these', 'they', 'theyd', 'they''ll', 'theyre', 'they''ve', 'think', 'this', 'those', 'thou', 'though', 'thoughh', 'thousand', 'throug', 'through',\n",
    "                   'thru', 'thus', 'til', 'tip', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'ts', 'twice', 'two', 'under',\n",
    "                   'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'ups', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using',\n",
    "                   'usually', 'various', 'very', 'via', 'viz', 'vol', 'vols', 'vs', 'want', 'wants', 'was', 'wasnt', 'way', 'we', 'wed', 'welcome', 'we''ll', 'went', 'were',\n",
    "                   'werent', 'we''ve', 'what', 'whatever', 'what''ll', 'whats', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres',\n",
    "                   'whereupon', 'wherever', 'whether', 'which', 'while', 'whim', 'whither', 'who', 'whod', 'whoever', 'whole', 'who''ll', 'whom', 'whomever', 'whos', 'whose',\n",
    "                   'why', 'widely', 'willing', 'wish', 'with', 'within', 'without', 'wont', 'words', 'world', 'would', 'wouldnt', 'www','yes', 'yet', 'you', 'youd', 'you''ll',\n",
    "                   'your', 'youre', 'yours', 'yourself', 'yourselves', 'you''ve', 'zero']\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def clean_jv(doc):\n",
    "   typo_free = \" \".join([(sym_spell.lookup(i, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)[0].term) for i in doc]) \n",
    "   twol_free = strip_short(typo_free)\n",
    "   return twol_free\n",
    "def clean(lst):\n",
    "    lst = [token for token in lst if not token.is_stop]\n",
    "    lst = [token.lemma_ for token in lst if str(token.lemma_) not in stops]\n",
    "    lst = [re.sub(r'[\\W\\d\\s]', '', string) for string in lst]\n",
    "    lst = [token for token in lst if token not in stops]\n",
    "    while '' in lst:\n",
    "        lst.remove('')\n",
    "    lst = clean_jv(lst)\n",
    "    return lst\n",
    "df['tokens'] = df['tokens'].map(lambda x: clean(x))\n",
    "#Creating a single-string version of the cleaned texts, for tools that require it.\n",
    "df['token_str'] = df['tokens'].map(lambda x: ' '.join(x))\n",
    "doc_clean = df['tokens'].map(lambda x: list(x.split()))\n",
    "bigram =gensim.models.Phrases(doc_clean, min_count = 5, threshold = 50)\n",
    "trigram =gensim.models.Phrases(bigram[doc_clean], min_count = 5, threshold = 50)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "def make_bigrams(texts):\n",
    "    return bigram_mod[texts]\n",
    "def make_trigrams(texts):\n",
    "    return trigram_mod[bigram_mod[texts]]\n",
    "doc_trigram = df['tokens'].map(lambda x: make_trigrams(x.split()))\n",
    "dct = corpora.Dictionary(doc_trigram)\n",
    "dct.filter_extremes(no_below=5, no_above=0.6666, keep_n=90000)\n",
    "doc_term_matrix = [dct.doc2bow(doc) for doc in doc_trigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what the topics are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3267375348097861"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "#Create tf-idf vectors\n",
    "tfidf = TfidfModel(corpus)\n",
    "vectors = tfidf[corpus]\n",
    "from gensim.models.nmf import Nmf\n",
    "nmf_model = Nmf(vectors, id2word = dct, num_topics=6, kappa=0.08, eval_every=500, normalize=True, passes=10, random_state=42)\n",
    "print(\"Here's what the topics are: \")\n",
    "nmf_model.print_topics()\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "cm = CoherenceModel(model=nmf_model, corpus=vectors, texts = doc_trigram, coherence='c_v')\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence # show coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"interior\" + 0.009*\"fast\" + 0.008*\"rich\" + 0.008*\"remodel\" + 0.007*\"glass\" + 0.007*\"vault\" + 0.007*\"spectacular\" + 0.007*\"afternoon\" + 0.006*\"backslash\" + 0.006*\"response\"'),\n",
       " (1,\n",
       "  '0.012*\"concrete\" + 0.011*\"great\" + 0.010*\"acclaim\" + 0.008*\"hot_water_heater\" + 0.008*\"fully\" + 0.007*\"quiet\" + 0.007*\"wood\" + 0.007*\"custom\" + 0.006*\"walkthrough\" + 0.006*\"convenient\"'),\n",
       " (2,\n",
       "  '0.015*\"suite\" + 0.015*\"reserve\" + 0.014*\"easy_commute\" + 0.014*\"fresh\" + 0.014*\"speed\" + 0.014*\"beach\" + 0.014*\"search\" + 0.014*\"consist\" + 0.013*\"electrical_plumb\" + 0.013*\"easy\"'),\n",
       " (3,\n",
       "  '0.011*\"live\" + 0.011*\"afternoon\" + 0.009*\"pane\" + 0.009*\"condition\" + 0.009*\"port\" + 0.008*\"deck\" + 0.008*\"large\" + 0.008*\"proof\" + 0.008*\"clean\" + 0.007*\"drawer\"'),\n",
       " (4,\n",
       "  '0.013*\"appliance\" + 0.013*\"update\" + 0.011*\"tile\" + 0.010*\"stave\" + 0.009*\"drawer\" + 0.009*\"central_air\" + 0.009*\"flow\" + 0.008*\"average\" + 0.008*\"carpet\" + 0.008*\"deadline\"'),\n",
       " (5,\n",
       "  '0.007*\"backyard\" + 0.007*\"long\" + 0.006*\"wood\" + 0.006*\"appliance\" + 0.006*\"garage\" + 0.006*\"lovingly_care\" + 0.006*\"single\" + 0.006*\"fully\" + 0.006*\"quiet\" + 0.005*\"sunroom\"')]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [charm, brick, update, feature, tile, tile, ca...\n",
       "1     [single, update, clean, white, appliance, incl...\n",
       "2     [beautiful, newly, renovate, open, space, conc...\n",
       "3     [north, ogden, beauty, feature, stun, mountain...\n",
       "4     [multiple, offer, receive, showing, time, read...\n",
       "5     [split, entry, quiet, jul_sac, wait, family, l...\n",
       "6     [remodel, perfect, condition, laminate, wood, ...\n",
       "7     [spectacular, remodel, great, location, large,...\n",
       "8     [price_reduce, beautifully, remodel, centervil...\n",
       "9     [layton, enjoy, open, space, ready, entertain,...\n",
       "10    [glen, eagle, golf_course, nearby, superb, upd...\n",
       "11    [upper, east, bench, price_reduce, maintain, b...\n",
       "12    [beautiful, mccall, build, soar, ceiling, beau...\n",
       "13    [park, backyard, love, great, sip_morning_coff...\n",
       "14    [dream, relax, porch, brand, fully, fence, acr...\n",
       "15    [great, east, bench, great, neighbourhood, squ...\n",
       "16    [large, vault, ceiling, upstairs, include, pri...\n",
       "17    [darling, east, layton, wonderful, plan, inclu...\n",
       "18    [location, location, location, amaze, cottage,...\n",
       "19    [custom, build, steve, cook, dallas, style, br...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_trigram[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-56dcd16617e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrigram_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'renovated'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'components' is not defined"
     ]
    }
   ],
   "source": [
    "trigram_mod.score_item('new', 'renovated', components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [charm, brick, update, feature, tile, tile, ca...\n",
       "1        [single, update, clean, white, appliance, incl...\n",
       "2        [beautiful, newly, renovate, open, space, conc...\n",
       "3        [north, ogden, beauty, feature, stun, mountain...\n",
       "4        [multiple, offer, receive, showing, time, read...\n",
       "                               ...                        \n",
       "24877    [brick, ranch, grow, family, need, grow, step,...\n",
       "24878    [newly, update, beautiful, half, house, oak, h...\n",
       "24879    [cute_button, describe, farmhouse, entryway, f...\n",
       "24880    [warmth, abound, price, estate, sale, hardwood...\n",
       "24881    [rarely_available, ranch, meticulously_maintai...\n",
       "Name: tokens, Length: 24882, dtype: object"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_trigram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
