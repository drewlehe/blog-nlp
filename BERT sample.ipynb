{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12240</th>\n",
       "      <td>1926378</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Aries</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>So, Michael Moore's new film 'Fahren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170857</th>\n",
       "      <td>3027884</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>03,May,2004</td>\n",
       "      <td>I had to go to Walmart this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253195</th>\n",
       "      <td>3849840</td>\n",
       "      <td>male</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>09,July,2004</td>\n",
       "      <td>Aww..i am leaving for Fl. tomorrow...do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278378</th>\n",
       "      <td>1169141</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>01,June,2003</td>\n",
       "      <td>BBQ and overkill   oH MAN, I jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291205</th>\n",
       "      <td>2929974</td>\n",
       "      <td>female</td>\n",
       "      <td>36</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>Leo</td>\n",
       "      <td>26,February,2004</td>\n",
       "      <td>Every year about this time I start thinking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender  age       topic       sign              date  \\\n",
       "12240   1926378  female   24      indUnk      Aries      09,June,2004   \n",
       "170857  3027884  female   33  Non-Profit   Aquarius       03,May,2004   \n",
       "253195  3849840    male   16     Student      Virgo      09,July,2004   \n",
       "278378  1169141    male   33        Arts  Capricorn      01,June,2003   \n",
       "291205  2929974  female   36  Publishing        Leo  26,February,2004   \n",
       "\n",
       "                                                     text  \n",
       "12240             So, Michael Moore's new film 'Fahren...  \n",
       "170857                   I had to go to Walmart this m...  \n",
       "253195         Aww..i am leaving for Fl. tomorrow...do...  \n",
       "278378                BBQ and overkill   oH MAN, I jus...  \n",
       "291205     Every year about this time I start thinking...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('blogtext.csv').sample(40000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['text'].astype(str)\n",
    "Y=pd.get_dummies(df['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19600,) (19600, 2)\n",
      "(8400,) (8400, 2)\n",
      "(12000,) (12000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 100)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train,Y_train, test_size = 0.30, random_state = 100)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_valid.shape,Y_valid.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19600,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "train_tokens = X_train.map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:64])\n",
    "test_tokens = X_test.apply(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:64])\n",
    "train_tokens_ids = pd.Series(train_tokens).apply(tokenizer.convert_tokens_to_ids)\n",
    "test_tokens_ids = pd.Series(test_tokens).apply(tokenizer.convert_tokens_to_ids)\n",
    "train_tokens_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do I need this step?\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=64, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=64, truncating=\"post\", padding=\"post\", dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "BATCH_SIZE = 64\n",
    "test_y = np.array(Y_test)\n",
    "train_y = np.array(Y_train)\n",
    "train_tokens_tensor = torch.LongTensor(train_tokens_ids)\n",
    "train_y_tensor = torch.from_numpy(train_y)\n",
    "test_tokens_tensor = torch.LongTensor(test_tokens_ids)\n",
    "test_y_tensor = torch.from_numpy(test_y)\n",
    "train_dataset = TensorDataset(train_tokens_tensor, train_y_tensor)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "test_dataset = TensorDataset(test_tokens_tensor, test_y_tensor)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()\n",
    "train_y_tensor\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMultipleChoice,\n",
    "    RobertaTokenizer,\n",
    "    XLNetConfig,\n",
    "    XLNetForMultipleChoice,\n",
    "    XLNetTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'log_softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-d01a06a7b93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mbert_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'log_softmax'"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, BertForSequenceClassification, BertModel\n",
    "EPOCHS=3\n",
    "con = BertConfig(vocab_size_or_config_json_file=30522)\n",
    "bert_clf = BertForSequenceClassification(config = con)\n",
    "optimizer = AdamW(bert_clf.parameters(), lr=3e-6)\n",
    "bert_clf.train()\n",
    "for epoch_num in range(EPOCHS):\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, labels = [t for t in batch_data]\n",
    "        labels = torch.autograd.Variable(labels)\n",
    "        probas = bert_clf(token_ids.to(torch.int64))\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0231, -0.0355],\n",
       "         [ 0.0405, -0.1860],\n",
       "         [ 0.4154,  0.0209],\n",
       "         [ 0.3506,  0.1075],\n",
       "         [ 0.3131, -0.0650],\n",
       "         [ 0.0087,  0.0698],\n",
       "         [ 0.2654, -0.0740],\n",
       "         [ 0.0859, -0.0813],\n",
       "         [ 0.1231, -0.1525],\n",
       "         [ 0.3443,  0.1602],\n",
       "         [ 0.0948,  0.1089],\n",
       "         [ 0.2314, -0.2188],\n",
       "         [ 0.0339, -0.0992],\n",
       "         [ 0.1025, -0.1835],\n",
       "         [ 0.3428,  0.0027],\n",
       "         [ 0.0770, -0.2052],\n",
       "         [ 0.4102,  0.0588],\n",
       "         [ 0.1848, -0.1553],\n",
       "         [ 0.1020, -0.3258],\n",
       "         [ 0.1746, -0.1525],\n",
       "         [ 0.3763, -0.0120],\n",
       "         [ 0.4577, -0.1242],\n",
       "         [ 0.3115, -0.0433],\n",
       "         [ 0.2976, -0.0897],\n",
       "         [ 0.0461, -0.0145],\n",
       "         [ 0.1704, -0.1950],\n",
       "         [ 0.1516, -0.2341],\n",
       "         [-0.0765,  0.0220],\n",
       "         [ 0.1270, -0.2870],\n",
       "         [ 0.2282,  0.0089],\n",
       "         [ 0.1738, -0.2086],\n",
       "         [ 0.1883, -0.2205],\n",
       "         [ 0.1445,  0.0703],\n",
       "         [ 0.2909, -0.2309],\n",
       "         [ 0.1515, -0.1391],\n",
       "         [ 0.1498, -0.1856],\n",
       "         [ 0.0065, -0.0505],\n",
       "         [ 0.1085, -0.1986],\n",
       "         [-0.0501, -0.1599],\n",
       "         [ 0.1898, -0.3814],\n",
       "         [ 0.1738,  0.0389],\n",
       "         [ 0.2440, -0.1012],\n",
       "         [ 0.1255, -0.0863],\n",
       "         [ 0.3543, -0.2288],\n",
       "         [ 0.1567, -0.0899],\n",
       "         [-0.0207, -0.1780],\n",
       "         [ 0.2916, -0.2711],\n",
       "         [ 0.1317, -0.1948],\n",
       "         [ 0.1991, -0.2529],\n",
       "         [ 0.0590, -0.2321],\n",
       "         [ 0.1853,  0.1035],\n",
       "         [ 0.1662, -0.0396],\n",
       "         [ 0.1405, -0.1348],\n",
       "         [ 0.0909,  0.0632],\n",
       "         [ 0.3333, -0.1589],\n",
       "         [ 0.0850, -0.3351],\n",
       "         [ 0.1091, -0.0836],\n",
       "         [ 0.4900, -0.0938],\n",
       "         [ 0.3131, -0.0379],\n",
       "         [ 0.2400, -0.2694],\n",
       "         [ 0.3458, -0.3084],\n",
       "         [ 0.0725,  0.0232],\n",
       "         [ 0.0147, -0.2484],\n",
       "         [ 0.1412,  0.0852]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My problem seems to be a shaping problem. Googling and looking at the PyTorch forums hasn't really helped.\n",
    "probas\n",
    "# probas.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
